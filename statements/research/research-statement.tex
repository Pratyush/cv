\newif\ifpc
\newif\iffull
\newif\ifnotes
\newif\iflater

\fulltrue
%\fullfalse

\notestrue
%\notesfalse

\latertrue
%\laterfalse

\pctrue
% \pcfalse

\synctex=1


\documentclass[11pt,letterpaper]{article}
\usepackage[notes=xxx,later=false]{../dtrt}
\usepackage[utf8]{inputenc}
\usepackage[backend=biber,style=alphabetic,maxnames=22,maxalphanames=6]{biblatex}
\usepackage[margin=0.8in]{geometry}
\DeclareFieldFormat[misc]{title}{\mkbibquote{#1\isdot}}
\AtEveryBibitem{%
  \ifentrytype{inproceedings}{%
    \clearfield{year}%
    \clearfield{pages}%
    \clearfield{note}%
  }{%
  }%
}
% Format bib entries
\AtEveryBibitem{%
  \ifentrytype{misc}{%
    \clearfield{year}%
  }{%
  }%
}
\AtEveryBibitem{%
  \ifentrytype{article}{%
    \clearfield{pages}%
    \clearfield{volume}%
    \clearfield{number}%
    \clearfield{month}%
  }{%
  }%
}



\bibliography{../title-long,../references}

% \usepackage[scr=euler,frak=pxtx]{mathalfa}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{times}
\usepackage{newtxtext}
\usepackage{microtype}
\usepackage{comment}
\usepackage{listings}
\usepackage{xspace}
\usepackage[inline,shortlabels]{enumitem}
  \setlist[itemize]{leftmargin=*}
  \setlist[enumerate]{leftmargin=*}
  \setlist[description]{leftmargin=*}
\usepackage{makecell}
\usepackage{tikz}
\usetikzlibrary{
matrix,
shapes,
shapes.geometric,
shapes.symbols,
shapes.arrows,
shapes.multipart,
shapes.callouts,
shapes.misc,
arrows,
positioning,
chains,
calc,
fit}
\usepackage{hypcap}
\usepackage{mleftright}
\usepackage[margin=4mm,footnotesize,labelfont=bf]{caption}

\usepackage{siunitx}
  \sisetup{group-minimum-digits=4,group-separator={,}}
\usepackage{bbm}
\usepackage{bm}
\usepackage{colortbl}
\usepackage{tabu}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{etoolbox}
\usepackage{graphicx}
\usepackage{breakcites}
\usepackage{booktabs}
\usepackage[f]{esvect}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{scalerel}
\usepackage{verbatimbox}
\usepackage{schemata}
\usepackage{float}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{color, soul}
\usepackage{subdepth}
\usepackage{varwidth}
\usepackage[capitalize, nameinlink]{cleveref}
\usepackage{xparse}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{setspace}
\usepackage{fp}
\usepackage{titling}
\setlength{\droptitle}{-3cm}

%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\NPRelation}{\mathcal{R}}
\newcommand{\NPInstance}{\mathbbm{x}}
\newcommand{\NPWitness}{\mathbbm{w}}
\newcommand{\NPIndex}{\mathbbm{i}}
\newcommand{\Proof}{\pi}
\newcommand{\Class}[1]{\mathsf{#1}}
\newcommand{\NP}{\Class{NP}}
\newcommand{\DoQuote}[1]{``#1''}
\newcommand{\mc}{\multicolumn}
\newcommand{\mr}{\multirow}
\newcommand{\defemph}[1]{\textbf{\emph{#1}}}
\newcommand{\doclearpage}{%
\iffull
\clearpage
\else
\fi
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{definition}[theorem]{Definition}

\newtheorem{itheorem}{Theorem}%[section]
\newtheorem{ilemma}{Lemma}%[section]
\newtheorem{assumption}{Assumption}
\newtheorem{idefinition}{Definition}%[section]
\newtheorem{icorollary}{Corollary}%[section]
\newtheorem{corollary}{Corollary}%[section]

\theoremstyle{definition} % not italics
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark} %
\newtheorem{case}{Case}

\crefname{assumption}{Assumption}{Assumptions}
\crefname{step}{Step}{Steps}
\crefname{claim}{Claim}{Claims}
\crefformat{footnote}{#2\footnotemark[#1]#3}
\crefmultiformat{footnote}{#2\footnotemark[#1]#3}{,~#2\footnotemark[#1]#3}{,~#2\footnotemark[#1]#3}{,~#2\footnotemark[#1]#3}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AUTHOR NOTES
%
\newcommand{\ale}[1]{\dtcolornote[Ale]{red}{#1}}
\newcommand{\imm}[1]{\dtcolornote[Ian]{purple}{#1}}
\newcommand{\mdg}[1]{\dtcolornote[Matt]{green}{#1}}
\newcommand{\pratyush}[1]{\dtcolornote[Pratyush]{blue}{#1}}
\newcommand{\benedikt}[1]{\dtcolornote[Benedikt]{yellow}{#1}}

\newcommand{\zexe}{\textsc{Zexe}}
\newcommand{\marlin}{\textsc{Marlin}}
\newcommand{\arkworks}{\texttt{arkworks}}

\edef\authorhash{\detokenize{7a31ae48766fecee5a26de15205949fc}}

\renewcommand{\mkbibnamegiven}[1]{%
  \iffieldequals{hash}{\authorhash}{\mkbibbold{#1}}{#1}}
\renewcommand{\mkbibnamefamily}[1]{%
  \iffieldequals{hash}{\authorhash}{\mkbibbold{#1}}{#1}}
\renewcommand{\mkbibnameprefix}[1]{%
  \iffieldequals{hash}{\authorhash}{\mkbibbold{#1}}{#1}}
\renewcommand{\mkbibnamesuffix}[1]{%
  \iffieldequals{hash}{\authorhash}{\mkbibbold{#1}}{#1}}

\newcommand{\hlauthor}[1]{\textcolor{blue}{{#1}}}

\DeclareFieldFormat{extraalpha}{\ifcategory{byname}{\mkbibbold{#1}}{#1}}
\pagestyle{plain}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% Declarations for Front Matter

\title{Research Statement: Systems with Strong Privacy and Integrity Guarantees via Verifiable Computation}
\author{Pratyush Mishra}
\date{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\maketitle
\vspace{-1cm}

Modern systems require users to trust service providers to ensure integrity of computation and for privacy of their data. 
This state of affairs is untenable, as it makes applications vulnerable to abuse by (e.g., censorship, or abuse of user data) and of (e.g., data breaches) service providers.

Cryptography provides powerful tools for reducing the trust required of service providers by limiting the data visible to them and by preventing them from deviating from their prescribed roles. 
For example, end-to-end-encryption with TLS or secure messaging systems has reduced the need to trust operators of internet infrastructure for privacy.

Unfortunately, existing deployments of cryptography, while successful at providing strong privacy and integrity guarantees for data at rest, offer little protection for modern applications which need to \emph{compute} on such data.
On the other hand, the theory of cryptography has developed powerful tools (e.g., multi-party computation) for exactly this purpose, but these tools generally incur high overheads and hence have seen limited deployment.

\textbf{My research} aims to resolve the foregoing issue by designing systems with strong efficiency, privacy, and integrity guarantees for computations on data by focusing on one promising primitive: \emph{verifiable computation} (VC).
Specifically, my work focuses on \emph{succinct zero-knowledge proofs}, or \emph{zkSNARKs}, a powerful form of verifiable computation that enables a prover to produce a short proof that convinces a verifier of the truth of a claim (e.g.: ``I know the preimage to this hash.''), while providing two properties. 
The first, \emph{zero-knowledge}, ensures that the proof hides all information about \emph{why} the claim is true (e.g.: what the preimage is). 
The second, \emph{succinctness}, ensures that the verifier's effort in checking the proof is much less than the cost of directly checking the claim (e.g.: directly computing the hash).

My research approach consists of three steps. 

First, I investigate where existing systems fall short.
After identifying such shortcomings, I construct and implement novel cryptographic protocols that address these. I aim to design my protocols in a modular and reusable manner that simplifies security analysis while simultaneously enabling better optimization opportunities.
Finally, I drive adoption of these protocols by implementing them in composable open-source libraries that efficiently realize the components necessary for many advanced cryptographic protocols.

This approach has resulted in significant impact via industrial adoption of my research and widespread usage of my open-source libraries.

In the rest of this document, I detail three thrusts of my research that follow the above approach.
In \cref{sec:low-memory-proving}, I describe my work on constructing zkSNARKs that, unlike most prior constructions, impose very low memory requirements during proof generation, and hence enable even weak devices to generate proofs for large claims.
Then, in \cref{sec:outsourcing}, I describe my work on enabling clients to outsource proof generation to powerful servers. This line of work complements the first by supporting computations that just cannot be performed on weak devices.
Finally, in \cref{sec:applications}, I describe my work on applying zkSNARKs to design systems that provide strong privacy and integrity guarantees without compromising on efficiency.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generating zkSNARKs in low memory environments}
\label{sec:low-memory-proving}

\parhead{Past work}
During my PhD, I made fundamental contributions to the theory and practice of zkSNARKs \cite{ChiesaHMMVW20, BunzCMS20, BunzCLMS21, BunzMMTV21}.
For example, my work on Marlin \cite{ChiesaHMMVW20} developed a novel methodology to construct zkSNARKs that underlies almost all modern zkSNARK constructions.

Unfortunately, almost all known SNARKs require the prover to store a large amount of intermediate state in memory during proof generation, and hence are not suitable for weak devices.
To resolve, this my students and I have developed new zkSNARK design and implementation techniques that enable low-memory proof generation.
We have taken two complementary approaches: \emph{monolithic} low-memory zkSNARKs, and \emph{incremental} low-memory zkSNARKs.
Monolithic zkSNARKs take as input the entire claim in one go, and produce a proof for it, while incremental zkSNARKs receive the claim in small chunks, and incrementally build up the proof.

\parhead{Monolithic low-memory zkSNARKs}
In recent work, we constructed Scribe (to be submitted to USENIX Security 2026), a low-memory SNARK that can efficiently prove large statements even on cheap consumer devices such as smartphones by leveraging a plentiful, but heretofore unutilized, resource: disk storage. 
In more detail, instead of storing its (large) intermediate state in RAM, Scribe's prover instead stores it on disk. 
To ensure that accesses to state are efficient, we designed Scribe's prover in a *read-write streaming* model of computation that allows the prover to read and modify its state only in a streaming manner.

We implemented and evaluated Scribe's prover, and showed that, on commodity hardware, it can easily scale to claims of size $2^{28}$ gates while using only 2GB of memory and incurring minimal proving latency overhead (10-35\%) compared to the memory-intensive baseline that requires much more memory. 
Our implementation minimizes overhead by leveraging the streaming access pattern to enable several systems optimizations that together mask I/O costs.

In follow-up work (to be submitted to TCC 2026), we are exploring low-memory provers that eliminate the need for disk storage, while still achieving low prover time overhead.
Our initial results in this regime indicate that we can get performance that is within 70\% of the memory-intensive baseline.
We have also been able to develop lower bounds that show that (under reasonable restrictions), the protocols we have developed achieve optimal space-time tradeoffs.

\parhead{Incremental low-memory zkSNARKs}
Prior highly-efficient constructions of low-memory SNARKs for incremental computations \cite{BunzCLMS21,KothapalliST22} rely on homomorphic vector commitments whose security is based on public-key assumptions.
This renders them vulnerable to quantum attacks, and also worsens their concrete efficiency.

In recent work \cite{BunzMNW25} (ITCS 2025), my collaborators and I constructed the first efficient incremental low-memory SNARKs from *non-homomorphic* vector commitments which can be realized from solely symmetric-key assumptions (e.g. Merkle trees).
We overcome the need for homomorphisms by instead performing spot-checks over error-correcting encodings of the committed vectors.

Unfortunately, unlike prior incremental low-memory SNARKs, the efficiency of our construction degrades logarithmically with the number of computation steps. 
In follow-up work Arc \cite{BunzMNW24} (submitted to CRYPTO 2025) we eliminate this degradation and obtain concretely efficient incremental low-memory SNARKs with plausible post-quantum security.
Underlying Arc is a novel \emph{accumulation scheme} \cite{BunzCLMS21,KothapalliST22} for claims about proximity of claimed codewords to the Reed--Solomon code. 
Unlike our prior work, this new scheme additionally supports accumulating claims up to list-decoding radius, resulting in concrete efficiency improvements.

While Arc greatly improves upon our prior work, its prover still requires quasilinear time in the size of the claim (due to the Reed--Solomon encoding).
My students and I have thus developed FACS \cite{BawejaMMS25} (submitted to CRYPTO 2025), a new accumulation scheme for claims about proximity of claimed codewords to \emph{linear-time} encodable codes.
Unlike Arc, FACS' prover runs in linear time in the size of the claim.
FACS relies on novel protocols for ``codeswitching'' tensor codes, and also develops a new notion of security that enables proofs of security for protocols that rely on linear-time encodable codes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outsourcing zkSNARK proof generation}
\label{sec:outsourcing}

In some cases, the proving device is simply too constrained (e.g., weak CPU) to generate proofs efficiently by itself.
Indeed, such a case could arise when the low-memory solutions described in \cref{sec:low-memory-proving} result in high latency.
I will now describe the protocols I have developed for handling such scenarios by outsourcing proof generation to powerful servers.
My students and I have developed new techniques for \emph{outsourcing} proof generation to powerful servers, both when privacy is required (to hide the prover's secret input), and when it is not.
 
\parhead{Privacy-preserving outsourcing}
In Eos \cite{ChiesaLMZ23} (USENIX Security 2023), we designed the first privacy-preserving proof delegation protocols for zkSNARKs. 
Eos' protocols enable a prover to outsource proof generation to a set of workers, so that if at least one worker does not collude with other workers, no private information is revealed to any worker. 

We implemented Eos' delegation protocols for a state-of-the-art zkSNARK, and 
our evaluation demonstrated that these protocols are concretely efficient: when compared to local proving on a recent smartphone, Eos (a) reduces end-to-end latency by up to $26\times$, (b) lowers the delegator's active computation time by up to $1447\times$, and (c) enables proving up to $256\times$ larger instances.

In follow-up work DFS \cite{HuMWXYZ25} (USENIX Security 2025), we extended Eos to additionally support \emph{distributed} proving within each worker. 
DFS co-designs the zkSNARK with the delegation protocol to maximize efficiency for distributed proving and minimize overhead for delegation.
Concretely, unlike Eos, which requires linear communication between workers, DFS achieves zero communication and further achieves malicious security for free. 
We again implemented and evaluated DFS, and demonstrated that it scales better than Eos and other prior work. For example, for moderately large claims, DFS' end-to-end communication is $\sim 10^6$ times smaller than Eos. 


\parhead{Public outsourcing}
My collaborators and I have also designed new zkSNARKs that are optimized for efficient distributed proving when privacy is not required.

Our first work in this direction, Hekaton \cite{RosenbergMHMM24} (ACM CCS 2024), is a zkSNARK that can scale to efficiently handle arbitrarily large claims. 
We constructed Hekaton via a new ``distribute-and-aggregate'' framework that breaks up large computations into small chunks, proves these chunks in parallel in a distributed system, and then aggregates the resulting chunk proofs into a single succinct proof. 
Hekaton's aggregation technique builds on my prior work \cite{BunzMMTV21}.
Underlying this framework is a new technique for efficiently handling data that is shared between chunks that we believe could be of independent interest.
  
We implemented a distributed prover for Hekaton, and evaluated its performance on a compute cluster. Hekaton achieves strong horizontal scalability and can prove large computations quickly: it can prove computations of size $2^{35}$  gates in under an hour, which is much faster than prior work.

We also extended DFS (mentioned above) to support distributed proving in this public outsourcing setting. Like Hekaton, DFS achieves strong horizontal scalability, while reducing overall compute and communication costs.
Furthermore, DFS' proof size and verification time are much smaller than Hekaton's.

\parhead{Future directions}
Our existing protocols and systems leave room for improvement in a number of areas.
On the private outsourcing front, we aim to design protocols that eliminate the need for multiple non-colluding workers, and instead only rely on a single \emph{untrusted} worker.
On the public outsourcing front, we aim to design distributed proving algorithms for a popular efficient and plausibly-post-quantum class of SNARKs called \emph{hash-based SNARKs} \cite{BenSassonCS16} that offer numerous advantages (e.g., no trusted setup, fast provers, etc.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Using zkSNARKs to improve integrity and privacy of applications}
\label{sec:applications}

\parhead{Prior work}
During my PhD, my coauthors and I built $\zexe$ (\emph{Zero knowledge EXEcution}) \cite{BoweCGMMW20}, a ledger-based system for \emph{decentralized private computation} that enables users to execute computations offline and subsequently produce publicly-verifiable transactions that attest to the correctness of these offline executions. 
$\zexe$ simultaneously provides privacy and succinctness for these executions: a transaction reveals \emph{no information} about the offline computation whose correctness it attests to, and, furthermore, can be validated in time that is \emph{independent} of the cost of the computation. 
Overall, $\zexe$ transactions with two inputs and two outputs are $968$ bytes and can be verified in tens of milliseconds, \emph{regardless of the offline computation}, and generating these transactions takes less than a minute (plus a time that grows with the offline computation). 

Since then, my group and I have worked to provide similarly strong integrity and privacy guarantees for a number of other applications. We detail some below.



\paragraph{Integrity for database queries.}
Consider a smart contract that wishes to make queries into historical blockchain data.
Owing to its limited resources, the smart contract cannot directly query the blockchain.
It must thus rely on a third party to provide the query result, but then this requires trusting the third party to provide the correct result.
This is an instance of the following general problem: a client wishes to query an external database, but wants strong integrity guarantees about the query results.

In ongoing work, we design a new verifiable computation protocol that proves the correctness of SQL queries executed against a pre-committed database. 
Our approach introduces a suite of specialized ``sub-SNARKs'' for different relational operators, and a composition framework that assembles these into a SNARK for the full query. 
We also design new query optimization techniques whose objective is to reduce query \emph{proving} times instead of query \emph{execution} times.
We design and implement both components, and preliminary benchmarks show that our prototype can prove moderately complex queries in tens of seconds on a commodity laptop, which is orders-of-magnitude faster than prior work \cite{ZhangGKPP17}.

\paragraph{Privacy-preserving proofs of document contents.}
Many applications require proving properties of structured data. 
For instance, privacy-preserving credential systems often parse non-private credentials to prove selective attributes, such as age. 
Existing state-of-the-art \cite{AngelIMSW24} only supports simple regex-based extraction techniques. This is often insufficient for the aforementioned applications, and so, in ongoing work, we develop protocols for parsing data encoded using context-free grammars, enabling richer and more expressive proofs over document contents. 
Our initial implementation and evaluation demonstrates that proofs of parsing complex structured documents, such as JSON, can be generated in tens of seconds.

\paragraph{Privacy-preserving proofs of program compilation.}
A core tenet of computer security is to avoid executing untrusted binaries, which may behave maliciously. 
However, this threat model changes if binaries are accompanied by short cryptographic proofs certifying their provenance and safety. 
For example, a proof that a binary was compiled from a memory-safe source program eliminates an entire class of vulnerabilities. 
In ongoing work, we develop techniques for proving that a binary is the result of correctly compiling source code in a subset of C to WebAssembly. 
Our SNARK construction supports all stages of compilation, including parsing, optimization, and code generation. 
To reduce proving costs, we rely on \emph{non-deterministic NP checkers}, which check correctness of each compiler stage's output rather than re-executing it. Implementation is ongoing.


% \parhead{Past work}
% During my PhD, I developed $\mathsf{Oblix}$ \cite{MishraPCCP18}, a novel encrypted search scheme that provides strong efficiency and privacy guarantees by provably hiding access patterns. $\mathsf{Oblix}$ relies on a combination of novel oblivious-access techniques and recent hardware enclave platforms (e.g., Intel SGX) \cite{McKeenABRSSS13}.
% We used $\mathsf{Oblix}$ to add privacy to a number of applications:
% private contact discovery for Signal, private retrieval of public keys for Key Transparency, and searchable encryption that hides access patterns and result sizes.

% \subsection{Better integrity and efficiency for private retrieval}

% \pratyush{Mention downside: trusted hardware.}

% Since then, I have worked to improve the efficiency and integrity guarantees of a particular class of private search schemes: those where the data is publicly available, but client queries into it must be kept private.
% Such \emph{Private Information Retrieval} (PIR) schemes have found numerous applications recently, including private web search \cite{Tiptoe}, private DNS queries \cite{blah}, and private auditing for web certificates \cite{SimplePIR}.
% Unfortunately, existing PIR schemes suffer from two limitations: unrealistic trust assumptions that can compromise privacy, and poor efficiency.
% In recent and ongoing work, my students and I have developed new approaches to resolve these issues.

% \parhead{Integrity for private retrieval}
% Almost all known PIR schemes assume that the database server behaves honestly and does not deviate from its role in the PIR protocol.
% However, this assumption is unrealistic: 
% not only are attacks that leverage misbehaving servers possible to learn client queries easy to implement, but, for many applications (e.g., private web search) service providers are incentivized to carry out such attacks as they stand to profit from learning query contents.

% To rectify this, recent work has proposed \emph{maliciously-secure PIR} (mPIR) \cite{AuthenticatedPIR} that strengthens the guarantees of semi-honest PIR by ensuring that even a misbehaving server cannot compromise client privacy via selective-failure attacks, and must answer every client query with respect to the same database.
% However, existing mPIR schemes are only known from specific cryptographic assumptions, and incur higher communication costs than their semi-honest counterparts.

% In our recent work \cite{FalkMS24} (in submission to CRYPTO 2025), we developed a new \emph{compiler} that transforms \emph{any} single-server PIR scheme into an mPIR scheme in a black-box manner with minimal overhead with no additional assumptions.
% Instantiating our compiler with appropriate base PIR schemes gives the first constructions of mPIR under assumptions such as Decisional Composite Residuosity, Quadratic Residuosity, and $\phi$-hiding.
% Efficiency-wise, for a database of size $N$, our compiler yields mPIR schemes with $O(N^\varepsilon)$ communication and $O(1)$ computation overhead for arbitrarily small $\varepsilon < 1$.
% In fact, by applying our compiler to double-efficient PIR \cite{LinMW23}, we can even achieve \emph{doubly-efficient} mPIR which requires only $\textrm{polylog}(N)$ communication and server and client computation. 
% In comparison, all prior mPIR constructions incur at least $\Omega(\sqrt{N})$ cost in all these metrics.

% \parhead{Efficiency of private retrieval}
% A key bottleneck in PIR is the server computation required to answer a client query: in almost all known constructions, for a database of size $N$, the server must perform $O(N)$ public-key operations.
% Doubly-efficient PIR \cite{constructions} addresses this by reducing this cost to sublinear in the database size by allowing the server to \emph{preprocess} the database.
% Unfortunately, known constructions of doubly-efficient PIR are of primarily theoretical interest, as their asymptotic improvements are offset by humongous constant-factor overheads \cite{player}.

% In ongoing work, we construct the first concretely efficient doubly-efficient PIR scheme with (slightly) sublinear server computation of $O(N/\log N)$ and minimal preprocessing overhead.
% Our scheme relies on extending prior techniques for preprocessing matrix-vector products, and applying these in novel ways to construct our PIR schemes.

\pratyush{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact and open source software}
\label{sec:impact}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Industrial deployment}
\label{sec:deployment}

Systems for decentralized private computation based on \textbf{$\zexe$} are being deployed by numerous blockchain projects  \cite{aleo, mir, aztec, anoma}, while other projects \cite{celo, penumbra} have adopted $\zexe$'s techniques for efficient recursive composition.
\textbf{$\marlin$} is seeing adoption in a number of blockchain projects that rely on universal-setup SNARKs \cite{aleo,horizen, HabockGD21}.
\textbf{PCD} based on atomic and split accumulation is being employed in applications such as succinct ledgers \cite{BonneauMRS20,mina-general} and verifiable delay functions \cite{BonehBBF18, filecoin-vdf}.
Finally, \textbf{GIPA}-based protocols are being deployed by Filecoin \cite{GaillyMN21} to aggregate over seven million proofs each day.
Overall, my research has been deployed in projects that have collectively raised over \emph{$49$ million} dollars in funding.
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Open source software}
\label{sec:oss}

\parhead{\arkworks{}: A Rust ecosystem for zkSNARKs}
The numerous academic and industrial projects that use zkSNARKs tend to hand-roll their own custom zkSNARK implementations, each with custom optimizations and improvements that are not shared with other implementations. This fragmentation has resulted in inefficiencies and wasted re-implementation effort.

This motivated my coauthors and I to develop \arkworks{},\footnote{\url{https://arkworks.rs}} an open-source Rust ecosystem for zkSNARK development that provides generic, efficient, and easy-to-use modules implementing all the components required for zkSNARK programming (including elliptic curves, state-of-the-art zkSNARKs, inner-pairing products, PCD, and more). To enable users and contributors to get started with these libraries, I have also developed helpful documentation and tutorials\footnote{\url{https://github.com/arkworks-rs/r1cs-tutorial/}}\footnote{\url{https://github.com/Pratyush/algebra-intro}}, and have started an active community support channel.

As a result of these efforts, the \arkworks{} ecosystem  has attracted a vibrant contributor community with over fifty unique contributors that have collectively contributed over 800 pull requests spanning $120,000$ lines of Rust code. \arkworks{} libraries have seen deployment in a number of state-of-the-art industry projects \cite{mina-general, celo, anoma, manta, EberhardtT18, Noir}, have been used by academic works to implement and benchmark their schemes \cite{BoweCGMMW20,ChiesaHMMVW20,ChenCDW20,BagheryPR20,BunzCLMS21,BunzMMTV19,ZhangX21,TyagiFBT21,ChuXZ21,FengQZDC21,GurkanJMMST21,KohlweissMSV21,IliadiMHSKFVL21}, and have been used to facilitate community hackathons\footnote{\url{https://zkhack.dev/}}.

\parhead{\textsc{Delphi} and \textsc{Muse}}
We have also open-sourced our implementations of \textsc{Delphi}\footnote{\url{https://github.com/mc2-project/delphi/}} and \textsc{Muse}\footnote{\url{https://github.com/mc2-project/muse}}. The latter was awarded an artifact evaluation badge at USENIX Security 2022.
}

% \section{Future work}
% \label{sec:future-work}

% I aim to work towards a future where privacy and scalability in decentralized systems is the default. In my graduate work, I started working towards this goal by developing and implementing new cryptographic tools, and collaborating with practitioners and researchers to push for adoption of these tools. I intend to use this work as the foundation for my future efforts. Some concrete directions I intend to pursue are as follows.

% \parhead{zkSNARKs for shared data}
% Existing zkSNARKs assume a single prover that knows all inputs to the proven computation. Unfortunately, this is incompatible with many applications of decentralized systems that split information across mutually distrusting parties. This motivates combining MPC and zkSNARK techniques to develop new zkSNARKs that enable \emph{multi-party proving}. I intend to develop novel protocols that enable this more efficiently than prior work \cite{SchoenmakersVV16, KanjalkarZGM21} by specializing MPC for zkSNARK proving. Preliminary ideas include exploiting the verifiable nature of zkSNARKs to minimize the need for expensive malicious-security techniques, and utilizing the fact that checking computations is usually cheaper than performing them.

% \parhead{Scaling up zkSNARKs}
% Despite great advances in the past few years, zkSNARK proving is still much slower than native execution. I intend to combine cryptography and system techniques to reduce this overhead. Some preliminary ideas include combining zkSNARKs for uniform (RAM) computations with zkSNARKs for circuit computations to obtain the best of both worlds, and developing efficient protocols for privacy-preserving delegation of zkSNARK provers to powerful but untrusted cloud machines. 

% \parhead{Proof-carrying data secure against quantum attacks}
% Existing efficient PCD constructions rely on assumptions that are broken by quantum computers, and prior constructions that plausibly resist quantum attacks \cite{ChiesaOS20} impose high overhead. An interesting direction to remedy this is to construct accumulation schemes for post-quantum zkSNARKs and to then use these to obtain efficient post-quantum PCD.

{
\setlength\bibitemsep{3pt}
\renewcommand*{\bibfont}{\small}
\printbibliography
}


\end{document}
